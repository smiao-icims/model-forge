# ModelForge v2.2.0 Release Notes

**Release Date**: January 27, 2025

## üéâ Overview

ModelForge v2.2.0 brings enhanced model metadata, improved telemetry, and a new quiet mode for automation. This release maintains full backward compatibility while providing powerful new capabilities for developers and automation engineers.

## ‚ú® Key Features

### 1. Enhanced Model Metadata (Opt-in)

Access comprehensive model information and capabilities through the new `EnhancedLLM` wrapper:

```python
from modelforge import ModelForgeRegistry

registry = ModelForgeRegistry()
llm = registry.get_llm("openai", "gpt-4o", enhanced=True)

# Access model capabilities
print(f"Context window: {llm.context_length:,} tokens")
print(f"Max output: {llm.max_output_tokens:,} tokens")
print(f"Supports functions: {llm.supports_function_calling}")
print(f"Supports vision: {llm.supports_vision}")

# Estimate costs before making calls
cost = llm.estimate_cost(input_tokens=5000, output_tokens=1000)
print(f"Estimated cost: ${cost:.4f}")

# Configure with validation
llm.temperature = 0.7  # Validated against model limits
llm.max_tokens = 2000  # Checked against max_output_tokens
```

### 2. Enhanced Telemetry Display

The test command now provides richer telemetry information:

```bash
modelforge test --prompt "Hello world"
```

Output includes:
- **Context window usage**: See how much of the model's context you're using
- **Token estimation**: Automatic estimation for providers that don't report usage
- **Model capabilities**: Function calling, vision support, etc.
- **Cost tracking**: Accurate cost estimation for all providers

Example telemetry output:
```
==================================================
üìä Telemetry Information
==================================================
Provider: openai
Model: gpt-4o
Duration: 1234ms

üìù Token Usage:
  Prompt tokens: 100
  Completion tokens: 50
  Total tokens: 150

üìä Context Window:
  Model limit: 128,000 tokens
  Used: 100 tokens (0.1%)
  Remaining: 127,900 tokens
  Max output: 16,384 tokens
  Capabilities: ‚úì Functions, ‚úì Vision

üí∞ Estimated Cost: $0.000750
==================================================
```

### 3. Quiet Mode for Automation

Perfect for QA engineers and automation scripts:

```bash
# Get just the response - no formatting, no logs, no telemetry
modelforge test --prompt "What is 2+2?" --quiet
# Output: 2 + 2 = 4

# Perfect for piping
echo "Is this valid JSON?" | modelforge test --quiet | jq .

# Use in scripts
RESPONSE=$(modelforge test --prompt "Generate ID" --quiet)
```

### 4. Improved Developer Experience

- **Cleaner output by default**: INFO logs suppressed without `--verbose`
- **Better error messages**: More context and helpful suggestions
- **Fixed telemetry in enhanced mode**: Callbacks properly propagated
- **Token estimation**: For providers that don't report usage (e.g., GitHub Copilot)

## üîß Technical Improvements

### EnhancedLLM Architecture
- Delegation pattern wraps any LangChain model
- <100ms overhead for metadata features
- Full serialization support maintained
- Backward compatible with all existing code

### Token Estimation
- Automatic estimation when providers don't report usage
- Uses approximation: 1 token ‚âà 4 characters
- Helps track costs for subscription-based services

### Logging Control
- `--verbose`: Shows all logs including DEBUG
- Default: Shows only WARNING and above
- `--quiet`: Suppresses all output except response

## üìä Migration Guide

### From v2.1.x

No breaking changes! To use new features:

```bash
pip install --upgrade model-forge-llm
```

#### Try Enhanced Features

```python
# Opt-in to enhanced features
llm = registry.get_llm(enhanced=True)

# Or set environment variable
export MODELFORGE_ENHANCED=true
```

#### Use Quiet Mode

```bash
# For automation scripts
modelforge test --prompt "Hello" --quiet > output.txt
```

### Backward Compatibility

All existing code continues to work unchanged:
- `enhanced=False` by default (opt-in feature)
- Future versions may enable by default
- Use `enhanced=False` to maintain current behavior

## üêõ Bug Fixes

- Fixed telemetry callback propagation in EnhancedLLM
- Fixed token estimation for providers without usage data
- Fixed test failures with mock callback handlers
- Improved pre-commit hook compliance

## üìö Documentation Updates

- Updated README with new features and examples
- Enhanced API documentation for EnhancedLLM
- Added quiet mode examples for automation
- Improved telemetry documentation

## üôè Acknowledgments

Thanks to all contributors who helped make this release possible!

## üì¶ Installation

```bash
pip install --upgrade model-forge-llm
```

## üîó Links

- [GitHub Repository](https://github.com/smiao-icims/model-forge)
- [PyPI Package](https://pypi.org/project/model-forge-llm/)
- [Documentation](https://github.com/smiao-icims/model-forge#readme)
- [Issue Tracker](https://github.com/smiao-icims/model-forge/issues)

## What's Next?

In future releases, we plan to:
- Make enhanced features the default (with opt-out)
- Add more provider-specific optimizations
- Expand model capability detection
- Improve streaming support across all providers

Happy coding! üöÄ
