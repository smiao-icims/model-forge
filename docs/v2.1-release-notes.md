# ModelForge v2.1.0 Release Notes

## Overview

ModelForge v2.1.0 introduces two major features that significantly enhance CI/CD integration and user experience: **Environment Variable Authentication** and **Streaming Support with Authentication**. This release focuses on zero-touch deployment capabilities and real-time streaming while maintaining full backward compatibility.

## 🚀 New Features

### Environment Variable Authentication
Perfect for CI/CD pipelines and production deployments where interactive authentication isn't possible.

- **Zero-Touch Authentication**: No manual login required in automated environments
- **Standard Environment Variables**:
  - `MODELFORGE_OPENAI_API_KEY` - OpenAI API key
  - `MODELFORGE_ANTHROPIC_API_KEY` - Anthropic API key
  - `MODELFORGE_GITHUB_COPILOT_ACCESS_TOKEN` - GitHub Copilot OAuth token
  - `MODELFORGE_GOOGLE_API_KEY` - Google API key
- **Provider Name Normalization**: Both `github-copilot` and `github_copilot` map to `GITHUB_COPILOT`
- **Precedence**: Environment variables override stored credentials automatically
- **Security**: No credentials stored in configuration files when using env vars

### Streaming Support with Authentication
Real-time response streaming with sophisticated authentication handling for long-running requests.

- **CLI Streaming**: New `--stream` flag for real-time output
- **Authentication Monitoring**: Automatic token validity checking during streams
- **OAuth Token Refresh**: Seamless token renewal for long responses
- **Error Recovery**: Automatic retry on authentication failures
- **Provider-Aware**: Handles different streaming behaviors per provider
- **Python API**: Full async streaming support with callbacks

## 💻 Usage Examples

### Environment Variable Authentication

```bash
# Set up environment variables (one-time setup)
export MODELFORGE_OPENAI_API_KEY="sk-..."
export MODELFORGE_GITHUB_COPILOT_ACCESS_TOKEN="ghu_..."

# Use immediately without authentication
modelforge config use --provider openai --model gpt-4o-mini
modelforge test --prompt "Hello, world!"

# Perfect for CI/CD pipelines
echo "Explain the codebase" | modelforge test --stream
```

### CLI Streaming

```bash
# Real-time streaming responses
modelforge test --prompt "Write a detailed story" --stream

# Stream to file
modelforge test --prompt "Explain quantum computing" --stream --output-file explanation.txt

# Stream with different providers
modelforge config use --provider ollama --model qwen3:1.7b
modelforge test --prompt "What is machine learning?" --stream  # Token-by-token

modelforge config use --provider github_copilot --model claude-3.7-sonnet
modelforge test --prompt "Explain AI" --stream  # Buffered chunks
```

### Python API Streaming

```python
from modelforge.registry import ModelForgeRegistry
from modelforge.streaming import stream, StreamWrapper

# Basic streaming
registry = ModelForgeRegistry()
llm = registry.get_llm()

async for chunk in stream(llm, "Write a story about AI"):
    print(chunk, end="", flush=True)

# Advanced streaming with auth handling
config_data, _ = registry._config_manager.get_config()
provider_data = config_data.get("providers", {}).get("github_copilot", {})

wrapper = StreamWrapper(llm, "github_copilot", provider_data)
async for chunk in wrapper.stream("Explain quantum computing",
                                  timeout=300.0,
                                  buffer_size=5):
    print(chunk, end="", flush=True)

# Stream to file
from modelforge.streaming import stream_to_file
from pathlib import Path

await stream_to_file(llm, "Write documentation",
                    Path("output.txt"),
                    provider_name="openai",
                    provider_data=provider_data)
```

### CI/CD Integration

```yaml
# GitHub Actions example
- name: Run AI Analysis
  env:
    MODELFORGE_OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
  run: |
    modelforge config add --provider openai --model gpt-4o-mini
    modelforge config use --provider openai --model gpt-4o-mini
    echo "Analyze this pull request" | modelforge test --stream > analysis.txt
```

## 🔧 Technical Features

### Streaming Architecture
- **StreamingAuthHandler**: Monitors token validity during streaming
- **StreamWrapper**: Provides auth-aware streaming interface
- **Async Support**: Full async/await compatibility
- **Timeout Handling**: Configurable timeouts with proper error handling
- **Progress Callbacks**: Monitor streaming progress and token counts

### Authentication Enhancements
- **Environment Variable Priority**: `env var > stored credentials > prompt`
- **Provider Normalization**: Consistent environment variable naming
- **Token Refresh**: Background token renewal during streaming
- **Error Recovery**: Automatic re-authentication on auth failures

### Provider-Specific Behavior
- **Ollama**: True token-by-token streaming
- **GitHub Copilot**: Buffered response chunks (API limitation)
- **OpenAI**: Standard streaming behavior
- **Google**: Provider-dependent streaming

## 📈 Compatibility & Migration

### Backward Compatibility
- All existing configurations continue to work unchanged
- No breaking changes to public APIs
- Environment variables are additive - existing auth methods remain

### For Existing Users
No changes required. To use new features:

1. **Add Environment Variables** (optional):
   ```bash
   export MODELFORGE_OPENAI_API_KEY="your-key"
   ```

2. **Try Streaming** (optional):
   ```bash
   modelforge test --prompt "Hello" --stream
   ```

### For CI/CD Users
Replace interactive authentication with environment variables:

```bash
# Old way (requires interaction)
modelforge auth login --provider openai --api-key $API_KEY

# New way (zero-touch)
export MODELFORGE_OPENAI_API_KEY=$API_KEY
# Authentication happens automatically
```

## 🧪 Testing & Quality

### Test Coverage
- **227 total tests** with new streaming and environment variable test suites
- **test_auth_env_vars.py**: Comprehensive environment variable authentication tests
- **test_streaming_simple.py**: Basic streaming functionality tests
- **Integration tests**: Real provider streaming behavior verification

### Code Quality
- All new code passes pre-commit hooks (ruff, mypy, formatting)
- Type annotations for all new functions and methods
- Exception handling with proper error chaining
- Logging improvements for debugging streaming issues

## 🚀 Performance

### Minimal Overhead
- Environment variable checks add <1ms to authentication
- Streaming authentication monitoring has negligible impact
- Async streaming implementation scales to large responses
- No performance regression from v2.0

### Streaming Efficiency
- Buffered output reduces system calls
- Configurable buffer sizes for different use cases
- Memory-efficient for large responses
- Background token refresh doesn't block streaming

## 🔮 What's Next

v2.1.0 completes Phase 1 of our v2.1 roadmap. Future phases will include:

- **Phase 2**: OpenTelemetry integration for observability
- **Phase 3**: Enhanced configuration with profiles and hyperparameters
- **Phase 4**: Advanced testing and model comparison features

See [specs/v2.1/](../specs/v2.1/) for detailed future plans.

## 🙏 Acknowledgments

Thanks to all users who requested CI/CD integration and streaming support. Your feedback drives ModelForge development.

## 📝 Full Details

- **Changelog**: [CHANGELOG.md](../CHANGELOG.md)
- **Specifications**: [specs/v2.1/](../specs/v2.1/)
- **GitHub Release**: [v2.1.0](https://github.com/smiao-icims/model-forge/releases/tag/v2.1.0)

---

**Ready to upgrade?**
```bash
pip install --upgrade model-forge-llm
```

**Environment variables taking precedence?** ✅
**Real-time streaming?** ✅
**Zero-touch CI/CD?** ✅
**Backward compatibility?** ✅
